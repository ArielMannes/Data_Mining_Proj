{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Mining Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ariel Mannes - 302229190 , Eliran Tal - 200680676 , "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: for the data exploration we used only the genders with 100% certenty. We found the common words for: Male ,Female and Brand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from nltk.probability import FreqDist\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from zmq.backend.cython import message\n",
    "import os\n",
    "os.environ[\"MKL_THREADING_LAYER\"] = \"GNU\"\n",
    "os.environ[\"KERAS_BACKEND\"] = \"theano\"\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer as keras_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "ps = PorterStemmer()\n",
    "stop_words = stopwords.words('english') + list(string.punctuation)\n",
    "\n",
    "## import dataset\n",
    "df = pd.read_csv('gender-classifier-DFE-791531.csv', encoding='latin1')\n",
    "df = shuffle(shuffle(shuffle(df)))\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Question 1\n",
    "\n",
    "\n",
    "## text pre-processing\n",
    "dist = df.groupby('gender').size().to_frame()\n",
    "\n",
    "\n",
    "# compute gender disterbution before text processing\n",
    "def compute_percentage(x):\n",
    "    pct = float(x / df['gender'].size) * 100\n",
    "    return round(pct, 2)\n",
    "\n",
    "\n",
    "dist['percentage'] = dist.apply(compute_percentage, axis=1)\n",
    "dist\n",
    "\n",
    "# remove points we're not confident about\n",
    "filtered = df.values.tolist()\n",
    "filtered = list(filter(lambda x: x[6] == 1.0 and x[5] != \"unknown\", filtered))\n",
    "\n",
    "# split our data to three smaller lists by gender\n",
    "males = list(filter(lambda x: x[5] == \"male\", filtered))\n",
    "females = list(filter(lambda x: x[5] == \"female\", filtered))\n",
    "brands = list(filter(lambda x: x[5] == \"brand\", filtered))\n",
    "\n",
    "\n",
    "def try_str(t):\n",
    "    try:\n",
    "        return str(t)\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "        # remove characters like \"'\" or \".\"\n",
    "\n",
    "\n",
    "def wordify(t):\n",
    "    t = t.replace(\"'\", \"\")\n",
    "    t = t.replace(\",\", \" \")\n",
    "    t = t.replace(\".\", \" \")\n",
    "    t = t.replace(\"!\", \" \")\n",
    "    t = t.replace(\"?\", \" \")\n",
    "    t = t.replace(\"&\", \" \")\n",
    "    t = t.replace(\"|\", \" \")\n",
    "    t = t.replace(\"/\", \"\")\n",
    "    t = t.replace(\";\", \" \")\n",
    "    t = t.replace(\":\", \" \")\n",
    "    t = t.replace(\"\\\\\", \"\")\n",
    "    t = t.replace(\"\\\\n\", \" \")\n",
    "    return t\n",
    "\n",
    "\n",
    "# combine descriptions and text tweets into a single list of 'words'\n",
    "def get_list_of_words(lst):\n",
    "    word_list = \" \".join(list(map(lambda x: try_str(x[10]), lst)))\n",
    "    word_list = \" \".join(list(map(lambda x: try_str(x[19]), lst)))\n",
    "    word_list = word_list.lower().split(\" \")\n",
    "    # word_list = wordify(word_list).split(\" \")\n",
    "    word_list = list(w for w in word_list if len(w) > 1 and w not in stop_words)\n",
    "    return word_list\n",
    "\n",
    "\n",
    "def word_distrebution(text, messeg, f):\n",
    "    ret_list = []\n",
    "    list_words = \" \".join(list(map(f, text)))\n",
    "    #list_words = \" \".join(list(map(lambda x: try_str(x[19]), text)))\n",
    "    list_words = list_words.lower().split(\" \")\n",
    "    # word_list = wordify(word_list).split(\" \")\n",
    "    list_words = list(w for w in list_words if len(w) > 1 and w not in stop_words)\n",
    "    #list_words = get_list_of_words(text)\n",
    "    word_dist = FreqDist(list_words)\n",
    "    feature_set = word_dist.most_common(4000)\n",
    "    word_dist = word_dist.most_common(10)\n",
    "    dist = pd.DataFrame.from_records(word_dist).transpose\n",
    "    print (messeg)\n",
    "    print dist\n",
    "\n",
    "    for word in feature_set:\n",
    "        ret_list.append(word[0])\n",
    "\n",
    "    return  ret_list\n",
    "\n",
    "\n",
    "\n",
    "male_features = word_distrebution(males, 'Male most frequent words', lambda x: try_str(x[10])+try_str(x[19]))\n",
    "\n",
    "print ()\n",
    "female_features = word_distrebution(females, 'Female most frequent words', lambda x: try_str(x[10])+try_str(x[19]))\n",
    "\n",
    "print ()\n",
    "brand_features = word_distrebution(brands, 'Brand most frequent words', lambda x: try_str(x[10])+try_str(x[19]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: we used :\n",
    "    naive base classifier -  \n",
    "                        without wordify 63.9%\n",
    "                        with wordify    63.6%\n",
    "    logistic regrassion   - \n",
    "                        without wordify 63.7%\n",
    "                        with wordify    63.9%\n",
    "                        \n",
    "    keras model with embadding & lstm  -\n",
    "          8 epoches , batch size =256  -66.76%\n",
    "          4 epoches ,  batch size =256  -63.3%\n",
    "          8 epoches, batch size =256  - 66.76%\n",
    "          8 epoches, batch size = 500 - 63.83%\n",
    "          8 epoches, batch size =100  - 64.96%\n",
    "          \n",
    "             \n",
    "      \n",
    "    \n",
    "    wordify is a function that removes special characters\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Question 2\n",
    "def find_features(top_words, text):\n",
    "    feature = {}\n",
    "    for word in top_words:\n",
    "        feature[word] = word in text.lower()\n",
    "    return feature\n",
    "\n",
    "def union(a, b, c):\n",
    "    first = (set(a) | set(b))\n",
    "    return list(first | set(c))\n",
    "\n",
    "top_words = union(male_features, female_features, brand_features)\n",
    "\n",
    "tweet_by_gender = (map(lambda x: (try_str(x[10])+ try_str(x[19]), x[5]),filtered))\n",
    "\n",
    "feature_set = [(find_features(top_words, line[0]), line[1]) for line in tweet_by_gender]\n",
    "training_set = feature_set[:int(len(feature_set)*4/5)]\n",
    "testing_set = feature_set[int(len(feature_set)*4/5):]\n",
    "\n",
    "# creating a naive bayes classifier\n",
    "NB_classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "accuracy = nltk.classify.accuracy(NB_classifier, testing_set)*100\n",
    "print(\"Naive Bayes Classifier accuracy =\", accuracy)\n",
    "NB_classifier.show_most_informative_features(20)\n",
    "\n",
    "# creating a logistic regression classifier\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "accuracy = nltk.classify.accuracy(LogisticRegression_classifier, testing_set)*100\n",
    "print(\"Logistic Regression classifier accuracy =\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning predictor\n",
    "    embadding in the first layer\n",
    "    lstm in the hidden. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating a neural network classifier\n",
    "neural_data_set = list(filter(lambda x: x[1] != \"brand\", tweet_by_gender))\n",
    "x = list(neural_data_set[i][0] for i in range (0, len(neural_data_set)))\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(list(neural_data_set[i][1] for i in range(0, len(neural_data_set))))\n",
    "\n",
    "\n",
    "max_words = 4000\n",
    "k_tokenizer = keras_token(num_words=max_words)\n",
    "k_tokenizer.fit_on_texts(x)\n",
    "x = k_tokenizer.texts_to_sequences(x)\n",
    "x = sequence.pad_sequences(x)\n",
    "\n",
    "# treat the labels as categories\n",
    "y = keras.utils.to_categorical(y, 2)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state= 42)\n",
    "\n",
    "\n",
    "\n",
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embed_dim,input_length = x.shape[1]))\n",
    "model.add(LSTM(lstm_out))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=8, batch_size=256)\n",
    "\n",
    "#4 epoches, batch 256 - acc - 63.3\n",
    "#8 epoches,batch 256 - acc - 66.76\n",
    "#8 epoches, batch 500 - acc - 63.83\n",
    "#8 epoches, batch 100 - acc -64.96\n",
    "\n",
    "# Final evaluation of the model\n",
    "validation_size = 500\n",
    "X_validate = X_test[-validation_size:]\n",
    "Y_validate = y_test[-validation_size:]\n",
    "X_test = X_test[:-validation_size]\n",
    "y_test = y_test[:-validation_size]\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "import json\n",
    "import unicodecsv as csv\n",
    "from nltk.probability import FreqDist\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "\n",
    "# Collect live data\n",
    "\n",
    "\n",
    "def getLiveData():\n",
    "    consumer_key = 'uW7b9X2txbXPgXXPjHNxk3yvZ'\n",
    "    consumer_secret = 'nxQ38FkZkkkf3OSrL1pZwYbRBXRhTeKVetQtpecxtOsW0R7Erz'\n",
    "    access_token = '33584667-xIa3A136SDuAk32JNqkZZYKqnwUFA2s30hKs8qUAp'\n",
    "    access_secret = '59nOtlV0fEuS9qeN0DeroMqsWQ45Qmcn0Os5IqmnNWDvd'\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "    class MyListener(StreamListener):\n",
    "        def on_data(self, data):\n",
    "            data2 = json.loads(data)\n",
    "            if not data2['entities']['urls']:\n",
    "                description = data2['user']['description']\n",
    "                if not isinstance(description, basestring):\n",
    "                    description = \"\"\n",
    "                text = description + ' ' + data2['text']\n",
    "                print(text)\n",
    "                try:\n",
    "                    with open('realTime.csv', 'a') as f:\n",
    "\n",
    "                        writer = csv.writer(f, dialect='excel', encoding='utf-8')\n",
    "                        writer.writerow([text])\n",
    "                        return True\n",
    "                except BaseException as e:\n",
    "                    print(\"Error on_data: %s\" % str(e))\n",
    "                return True\n",
    "\n",
    "        def on_error(self, status):\n",
    "            print(status)\n",
    "            return True\n",
    "\n",
    "    twitter_stream = Stream(auth, MyListener())\n",
    "    twitter_stream.filter(languages=['en'],\n",
    "                          track=['nail', 'nails', 'feminism', '#nails', '#feminism', '#female', '#princess',\n",
    "                                 '#feelings',\n",
    "                                 '#aprincess', '#fashiondesigner', '#future', '#passion', '#dedication', '#year',\n",
    "                                 '#milliondollarlisting', '#fetuses', '#china', '#death', '#born', '#policy', '#info',\n",
    "                                 '#didyouknow', '#instagram', '#insta', '#instagood', '#instafact', '#instapic',\n",
    "                                 '#like4like', '#likeforlike', '#tag', '#tagsforlikes', '#instatag', '#instame'])\n",
    "\n",
    "\n",
    "def readData():\n",
    "    with open('realTime.csv') as f:\n",
    "        allData = []\n",
    "        reader = csv.reader(f, encoding='utf-8')\n",
    "        for row in reader:\n",
    "            allData.append(row[0])\n",
    "\n",
    "        return allData\n",
    "\n",
    "\n",
    "stop_words = stopwords.words('english') + list(string.punctuation) + ['RT', 'rt', '&amp;']\n",
    "\n",
    "\n",
    "def word_distrebution(text, messeg, f):\n",
    "    ret_list = []\n",
    "    list_words = \" \".join(list(map(f, text)))\n",
    "    list_words = list_words.lower().split(\" \")\n",
    "    list_words = list(w for w in list_words if len(w) > 1 and w not in stop_words)\n",
    "    word_dist = FreqDist(list_words)\n",
    "    feature_set = word_dist.most_common(4000)\n",
    "    word_dist = word_dist.most_common(10)\n",
    "    dist = pd.DataFrame.from_records(word_dist).transpose\n",
    "    print (messeg)\n",
    "    print dist\n",
    "\n",
    "    for word in feature_set:\n",
    "        ret_list.append(word[0])\n",
    "\n",
    "    return ret_list\n",
    "\n",
    "\n",
    "i = readData()\n",
    "\n",
    "word_distrebution(i, \"real time twits\", lambda x: x)\n",
    "print len(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: we took hashtags that we think that women use more than men and let our model to predict\n",
    "    our model predicted that out of the 4400 tweets that we collected 52.6% are women.\n",
    "    Since we didnt tagged the collected tweets its hard to draw solid conclusions, bud the model predicted that there are           more women than men.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# q 4  - testint the model on data that was collected in real time\n",
    "\n",
    "\n",
    "raw_tweets = pd.read_csv('realTime.csv', encoding='utf8')\n",
    "list_tweets =  list(raw_tweets.values[i][0] for i in range (0, len(raw_tweets.values)))\n",
    "\n",
    "#\n",
    "to_predict = k_tokenizer.texts_to_sequences(list_tweets)\n",
    "to_predict = sequence.pad_sequences(to_predict, maxlen=54L)\n",
    "\n",
    "\n",
    "predictions = model.predict(to_predict, batch_size = 200)\n",
    "\n",
    "#round predictions were 0 is female\n",
    "                     #  1 is male\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "num_of_men = rounded.count(1.0)\n",
    "num_of_women = len(rounded) - num_of_men\n",
    "print('women percentage in collected tweets', float(num_of_women)/len(rounded)*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
